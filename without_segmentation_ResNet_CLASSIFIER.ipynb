{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHUa7VAMV_rY",
        "outputId": "229b5dfb-1fca-4cf5-a3ec-c1910db255a9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: audiomentations in /usr/local/lib/python3.9/dist-packages (0.29.0)\n",
            "Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from audiomentations) (1.10.1)\n",
            "Requirement already satisfied: librosa<0.10.0,>0.7.2 in /usr/local/lib/python3.9/dist-packages (from audiomentations) (0.9.2)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.9/dist-packages (from audiomentations) (1.22.4)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.1.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.7.0)\n",
            "Requirement already satisfied: numba>=0.45.1 in /usr/local/lib/python3.9/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.56.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.9/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (23.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.9/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (3.0.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.4.2)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.9/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.9/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.45.1->librosa<0.10.0,>0.7.2->audiomentations) (63.4.3)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.45.1->librosa<0.10.0,>0.7.2->audiomentations) (0.39.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2.27.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (3.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.19.1->librosa<0.10.0,>0.7.2->audiomentations) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile>=0.10.2->librosa<0.10.0,>0.7.2->audiomentations) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa<0.10.0,>0.7.2->audiomentations) (2.21)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.9/dist-packages (0.9.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.4.2)\n",
            "Requirement already satisfied: numba>=0.45.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.22.4)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (23.0)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.9/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.7.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.9/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.45.1->librosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.45.1->librosa) (63.4.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa) (2.27.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa) (3.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (0.56.4)\n",
            "Requirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.9/dist-packages (from numba) (1.22.4)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba) (63.4.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install audiomentations\n",
        "!pip install librosa\n",
        "!pip install numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqHhgujTqnFA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import os\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#output_size = (input_size - kernel_size + 2*padding) / stride + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YO689K2IkcY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bayjnVRLoxj"
      },
      "outputs": [],
      "source": [
        "!cd drive/MyDrive/'472 Group project'/Project/Datasets && pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxv5ao-eOcpe"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/472 Group project/Project/Datasets'\n",
        "\n",
        "!cd path && ls\n",
        "\n",
        "os.chdir(path)\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2pRDnhpqxk9"
      },
      "outputs": [],
      "source": [
        "# #this allowed me to download dataset directly to google drive\n",
        "\n",
        "# !cd /content/drive/MyDrive/CS472/FinalTesting/DataSets && wget https://zenodo.org/record/1290750/files/IRMAS-TrainingData.zip?download=1\n",
        "# !cd /content/drive/MyDrive/CS472/FinalTesting/DataSets && mv 'IRMAS-TrainingData.zip?download=1' IRMAS-TrainingData.zip && unzip IRMAS-TrainingData.zip && rm -r IRMAS-TrainingData.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX9PXgbuBDNG"
      },
      "outputs": [],
      "source": [
        "! pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzLJGmn_4WOK"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYN3t4bCwEup"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_wf_sr(file_name):\n",
        "    effects = [[\"remix\", \"1\"]]\n",
        "    return torchaudio.sox_effects.apply_effects_file(file_name, effects=effects)\n",
        "\n",
        "\n",
        "def convert_waveform_to_mfcc(waveform, sr):\n",
        "    n_fft = 2048\n",
        "    win_length = None\n",
        "    hop_length = 512\n",
        "    n_mels = 256\n",
        "    n_mfcc = 128\n",
        "    mfcc_transform = T.MFCC(\n",
        "        sample_rate=sr,\n",
        "        n_mfcc=n_mfcc,\n",
        "        melkwargs={\n",
        "          'n_fft': n_fft,\n",
        "          'n_mels': n_mels,\n",
        "          'hop_length': hop_length,\n",
        "          'mel_scale': 'htk',\n",
        "        }\n",
        "    )\n",
        "    return mfcc_transform(waveform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiDZQiPDlgSI"
      },
      "outputs": [],
      "source": [
        "class Cycler:\n",
        "    def __init__(self):\n",
        "      self.c = []\n",
        "      self.i = 0\n",
        "\n",
        "\n",
        "    def fromList(self, c_list):\n",
        "        self.c = c_list\n",
        "        self.i = 0\n",
        "\n",
        "    def next(self):\n",
        "      if len(self.c) <= self.i:\n",
        "        return None\n",
        "      else:\n",
        "        item = self.c[self.i]\n",
        "        self.i += 1\n",
        "        return item\n",
        "\n",
        "    def skip(self, num):\n",
        "      self.i += num\n",
        "\n",
        "    def reset(self):\n",
        "      self.i = 0\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      return self.c[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.c)\n",
        "\n",
        "\n",
        "    def toList(self):\n",
        "        return self.c\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rv0cNxxGDcCF"
      },
      "outputs": [],
      "source": [
        "from re import I\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import librosa\n",
        "import math\n",
        "import random\n",
        "import audiomentations as A\n",
        "import wave\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from scipy.io import wavfile as wav\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class IRMASDataset(Dataset):\n",
        "    def __init__(self, root_dir, included_labels = [], segment_percent= .1, n_fts=128, train_percentage= .9, noise_percentage= .1, use_random=True):\n",
        "        self.root_dir = root_dir\n",
        "        self.use_random = use_random #this won't work if its not random 155\n",
        "\n",
        "        self.included_labels = included_labels\n",
        "\n",
        "\n",
        "        # shift will acquire the number of splits for audio file\n",
        "        self.shift = int(1/ segment_percent) # also num_features\n",
        "        self.segment_percent = segment_percent\n",
        "        self.num_features = n_fts\n",
        "\n",
        "        # datastructures\n",
        "        self.file_list = []\n",
        "        self.instrument_dict = {}\n",
        "        self.train_data_dict = {}\n",
        "        self.val_data_dict = {}\n",
        "        self.test_data_dict = {}\n",
        "        self.val_seg_list = []\n",
        "        self.test_seg_list = []\n",
        "        self.train_seg_list = []\n",
        "\n",
        "        self.first_file = None\n",
        "\n",
        "        #Information for dataloader partitioning\n",
        "        self.val_percentage = .3 * train_percentage #change this to increase validation percentage\n",
        "        self.train_percentage = train_percentage - (self.val_percentage)# change this to change everything else\n",
        "        self.test_percentage = 1 - train_percentage\n",
        "\n",
        "        #Noise augmentation\n",
        "        self.noise_aug = A.Compose([A.AddGaussianSNR(p=noise_percentage,\n",
        "                                              min_snr_in_db=10,\n",
        "                                              max_snr_in_db=20)])\n",
        "        #initialization functions\n",
        "        self.initialize_data_structs()\n",
        "        self.separate_train_test()\n",
        "\n",
        "        #data structures/values that need to be initialized after above functions\n",
        "        self.train_cut_off = int(len(self.file_list) * self.train_percentage)\n",
        "        self.val_cut_off = int(len(self.file_list) * self.val_percentage) + self.train_cut_off\n",
        "    def initialize_data_structs(self):\n",
        "        instrument_dict = {}\n",
        "        i = 0\n",
        "        for root, dirs, files in os.walk(self.root_dir):\n",
        "            for file in files:\n",
        "                if file.endswith('.wav'):\n",
        "                    file_name = os.path.join(root, file)\n",
        "                    parent_dir = Path(file_name).parent.name\n",
        "                    if parent_dir not in self.instrument_dict and parent_dir in self.included_labels:\n",
        "                        self.instrument_dict[parent_dir] = i\n",
        "                        i += 1\n",
        "                        self.train_data_dict[parent_dir] = []\n",
        "                        self.train_data_dict[parent_dir].append(file_name); self.file_list.append(file_name)\n",
        "                    elif parent_dir in self.instrument_dict:\n",
        "                        self.train_data_dict[parent_dir].append(file_name); self.file_list.append(file_name)\n",
        "        if self.use_random:\n",
        "            for instrument in self.train_data_dict:\n",
        "               random.shuffle(self.train_data_dict[instrument])\n",
        "            random.shuffle(self.file_list)\n",
        "\n",
        "    def separate_train_test(self):\n",
        "        for instrument in self.train_data_dict:\n",
        "          train_cut_off = int(len(self.train_data_dict[instrument]) * self.train_percentage)\n",
        "          val_cut_off = int(len(self.train_data_dict[instrument]) * self.val_percentage) + train_cut_off\n",
        "          q1 = Cycler(); q1.fromList(self.train_data_dict[instrument][:train_cut_off])\n",
        "          q2 = Cycler(); q2.fromList(self.train_data_dict[instrument][train_cut_off + 1:val_cut_off])\n",
        "          q3 = Cycler(); q3.fromList(self.train_data_dict[instrument][val_cut_off +1:])\n",
        "          self.train_data_dict[instrument] = q1\n",
        "          self.val_data_dict[instrument] = q2\n",
        "          self.test_data_dict[instrument] = q3\n",
        "\n",
        "    def get_segment_list(self, target_dict):\n",
        "        segment_list = []\n",
        "        for instrument in target_dict:\n",
        "            file_name = target_dict[instrument].next()\n",
        "            if file_name == None:\n",
        "                continue\n",
        "            if self.first_file == None:\n",
        "                self.first_file = file_name\n",
        "            label = self.instrument_dict[instrument]\n",
        "            label = torch.tensor(label)\n",
        "            audio, sr = librosa.load(file_name, sr=None)\n",
        "            mfcc = librosa.feature.mfcc(y = audio, sr=sr, n_mfcc=self.num_features)\n",
        "            mfcc = librosa.power_to_db(mfcc, ref=np.max) #am trying this out\n",
        "            mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n",
        "            mfcc = torch.tensor(mfcc, dtype=torch.float32)\n",
        "            num_frames = mfcc.shape[1]\n",
        "            segment_length = int(num_frames * self.segment_percent)  # number of frames per chunk\n",
        "\n",
        "            # split the tensor into segments of length segment_length along the second dimension (i.e., dim=1)\n",
        "            mfcc_frames_tensor = torch.chunk(mfcc, chunks= mfcc.shape[1] // segment_length, dim=1)\n",
        "            for frame in mfcc_frames_tensor:\n",
        "                if frame.shape[1] == segment_length and frame.shape[0] == self.num_features:\n",
        "                    segment = frame.clone().detach().float().unsqueeze(0)\n",
        "                    segment_list.append( (segment, label) )\n",
        "\n",
        "        if len(segment_list) == 0:\n",
        "            self.reset_cycles(target_dict)\n",
        "            return self.get_segment_list(target_dict)\n",
        "        return segment_list\n",
        "\n",
        "    def get_first_file(self):\n",
        "      return self.first_file\n",
        "\n",
        "    def reset_cycles(self, target_dict):\n",
        "      for instrument in target_dict:\n",
        "        target_dict[instrument].reset()\n",
        "\n",
        "\n",
        "    def full_reset(self):\n",
        "      self.train_seg_list.clear()\n",
        "      self.val_seg_list.clear()\n",
        "      self.test_seg_list.clear()\n",
        "      self.reset_cycles(self.train_data_dict)\n",
        "      self.reset_cycles(self.val_data_dict)\n",
        "      self.reset_cycles(self.test_data_dict)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx = idx // self.shift\n",
        "        if idx <= self.train_cut_off:\n",
        "          if len(self.train_seg_list) == 0:\n",
        "            self.train_seg_list = self.get_segment_list(self.train_data_dict)\n",
        "          mfcc_tensor, label_tensor = self.train_seg_list.pop()\n",
        "          return mfcc_tensor, label_tensor\n",
        "        elif idx > self.train_cut_off and idx <= self.val_cut_off:\n",
        "          if len(self.val_seg_list) == 0:\n",
        "            self.val_seg_list = self.get_segment_list(self.val_data_dict)\n",
        "          mfcc_tensor, label_tensor = self.val_seg_list.pop()\n",
        "          return mfcc_tensor, label_tensor\n",
        "        else:\n",
        "          if len(self.test_seg_list) == 0:\n",
        "            self.test_seg_list = self.get_segment_list(self.test_data_dict)\n",
        "          mfcc_tensor, label_tensor = self.test_seg_list.pop()\n",
        "          return mfcc_tensor, label_tensor\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.file_list) * self.shift\n",
        "\n",
        "    def increment_cycles(self, splits):\n",
        "      for inst in self.train_data_dict:\n",
        "        increment = len(self.train_data_dict[inst]) // splits\n",
        "        self.train_data_dict[inst].skip(increment)\n",
        "      for inst in self.val_data_dict:\n",
        "        increment = len(self.val_data_dict[inst]) // splits\n",
        "        self.val_data_dict[inst].skip(increment)\n",
        "      for inst in self.test_data_dict:\n",
        "        increment = len(self.test_data_dict[inst]) // splits\n",
        "        self.test_data_dict[inst].skip(increment)\n",
        "\n",
        "    def get_label(self, filename):\n",
        "        return self.instrument_dict[Path(filename).parent.name]\n",
        "\n",
        "    def get_number_of_labels(self):\n",
        "        return len(self.instrument_dict)\n",
        "\n",
        "labels = [\"sax\", \"tru\", \"vio\", \"voi\", \"cel\", \"cla\", \"flu\", \"gac\", \"gel\", \"org\", \"pia\"]\n",
        "\n",
        "IRMAS_ds = IRMASDataset(\"IRMAS-TrainingData\", included_labels= labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6gy9VlW5zYN"
      },
      "outputs": [],
      "source": [
        "# # used to combine\n",
        "def collate_fn(batch):\n",
        "  inputs, targets = zip(*batch) #separtes data and target tensors into separate lists. Target tensors are the labels\n",
        "  inputs_padded = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True) #padd the batches with zero if they are not filled\n",
        "  targets_padded = torch.stack(targets) #padds the targets with 0's until it is filled\n",
        "  return inputs_padded, targets_padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Qm0iDyqC8C7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Sampler\n",
        "class StartIndexSampler(Sampler):\n",
        "    def __init__(self, start_index, end_index):\n",
        "        self.start_index = start_index\n",
        "        self.end_index = end_index\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(range(self.start_index, self.end_index))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.end_index - self.start_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-BfK95_z2cl"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, SubsetRandomSampler, SequentialSampler, Subset\n",
        "\n",
        "import copy\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "def split_samples(dataset, num_splits=10, batch_size = 64, num_workers= 2):\n",
        "  num_files = len(dataset)\n",
        "  dataset.full_reset()\n",
        "\n",
        "  dataset_copies = []\n",
        "  train_dsLoader_list = []\n",
        "  val_dsLoader_list = []\n",
        "  test_dsLoader_list = []\n",
        "  train_percentage = dataset.train_percentage\n",
        "  val_percentage = dataset.val_percentage\n",
        "  test_percentage = dataset.test_percentage\n",
        "\n",
        "  train_start = 0\n",
        "  val_start = dataset.train_cut_off * dataset.shift + dataset.shift\n",
        "  test_start = dataset.val_cut_off * dataset.shift + dataset.shift\n",
        "\n",
        "\n",
        "  train_increment = int(train_percentage* num_files // num_splits)\n",
        "  val_increment = int(val_percentage * num_files // num_splits)\n",
        "  test_increment = int(test_percentage * num_files // num_splits)\n",
        "\n",
        "  train_prev = train_start\n",
        "  val_prev = val_start\n",
        "  test_prev = test_start\n",
        "\n",
        "  train_next = train_prev + train_increment\n",
        "  val_next = val_prev + val_increment\n",
        "  test_next = test_prev + test_increment\n",
        "  print(f\"split {num_files} inputs by {num_splits} splits: train size= {train_increment}, validation size= {val_increment}, test size= {test_increment}\")\n",
        "  skip_len = 0\n",
        "  current_dataset = copy.deepcopy(dataset)\n",
        "  for i in range(num_splits):\n",
        "      train_dsLoader_list.append( DataLoader(current_dataset,\n",
        "                                            num_workers=num_workers,\n",
        "                                            batch_size= batch_size,\n",
        "                                            sampler= StartIndexSampler(train_prev, train_next),\n",
        "                                            collate_fn= collate_fn\n",
        "                                            ) )\n",
        "      train_prev = train_next\n",
        "      train_next += train_increment\n",
        "      #print(val_prev, val_next)\n",
        "      val_dsLoader_list.append(DataLoader(current_dataset,\n",
        "                                          num_workers=num_workers,\n",
        "                                          batch_size= batch_size,\n",
        "                                          sampler= StartIndexSampler(val_prev, val_next),\n",
        "                                          collate_fn= collate_fn\n",
        "                                          ) )\n",
        "      val_prev = val_next + 1\n",
        "      val_next += val_increment\n",
        "      test_dsLoader_list.append( DataLoader(current_dataset,\n",
        "                                            num_workers=num_workers,\n",
        "                                            batch_size= batch_size,\n",
        "                                            sampler= StartIndexSampler(test_prev, test_next),\n",
        "                                            collate_fn= collate_fn\n",
        "                                            ) )\n",
        "      test_prev = test_next + 1\n",
        "      test_next += test_increment\n",
        "      dataset_copies.append(current_dataset)\n",
        "      current_dataset = copy.deepcopy(current_dataset)\n",
        "      current_dataset.increment_cycles(num_splits)\n",
        "\n",
        "  return train_dsLoader_list, val_dsLoader_list, test_dsLoader_list, dataset_copies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0E5f2k-z92Q"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import dataclass_transform\n",
        "batch_size = 1\n",
        "train_dsLoader_list, val_dsLoader_list, test_dsLoader_list, dataset_copies = split_samples(IRMAS_ds, num_splits = 1, batch_size= batch_size) #dataset, num_workers =2, num_splits, batch_size = 2\n",
        "\n",
        "\n",
        "#data, label = next(iter(test_dsLoader_list[0]))\n",
        "# data1, label1 = next(iter(train_dsLoader_list[5]))\n",
        "# print(data[0][0][30])\n",
        "# print(data1[0][0][30])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTkD1S5z1LrQ"
      },
      "source": [
        "Test to show the data sets are not equal, each time it a dataloader\n",
        "is called it copies the current dataset state it is assigned to by\n",
        "making copies it refers to the state of the copy.\n",
        "\n",
        "Copy of output:\n",
        "\n",
        "tensor([0.1227, 0.0205, 0.0053, 0.1575, 0.1030, 0.0833, 0.1713, 0.1905, 0.1395, 0.1874, 0.1687, 0.0704, 0.0523])\n",
        "\n",
        "tensor([-0.4809, -0.5016, -0.5025, -0.4433, -0.3915, -0.3718, -0.2472, -0.2602, -0.2802, -0.2363, -0.1246, -0.1997, -0.1865])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8Idr1frfj2D"
      },
      "outputs": [],
      "source": [
        "sample_idx = 0 # index of the sample to retrieve\n",
        "print(f\"Number of instruments: {len(IRMAS_ds.instrument_dict)}\\n\")\n",
        "#print(IRMAS_ds.data_dict.keys())\n",
        "print(f\"Shape of output: {IRMAS_ds[0][0].shape} \\n\")  #torch.Size([1, 50, 13])\n",
        "print(f\"Number of wave files: {len(IRMAS_ds) // 10}\\n\")\n",
        "\n",
        "#if you set segment_percentage to .1 the resulting shape will be 26 by 26.\n",
        "\n",
        "\n",
        "train_keys = \"Train keys: \"\n",
        "for key in IRMAS_ds.train_data_dict:\n",
        "  train_keys += (f\" {key}:{len(IRMAS_ds.train_data_dict[key])}\")\n",
        "print(train_keys)\n",
        "\n",
        "val_keys = \"\\nVal keys: \"\n",
        "for key in IRMAS_ds.train_data_dict:\n",
        "  val_keys += (f\" {key}:{len(IRMAS_ds.val_data_dict[key])}\")\n",
        "print(val_keys)\n",
        "\n",
        "test_keys = \"\\nTest keys: \"\n",
        "for key in IRMAS_ds.test_data_dict:\n",
        "  test_keys += f\" {key}:{len(IRMAS_ds.test_data_dict[key])} \"\n",
        "print(test_keys)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqBa_lJzWDLd"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "file_name = IRMAS_ds.file_list[random.randint(0, len(IRMAS_ds.file_list) - 1)]\n",
        "# load audio file\n",
        "audio, sr = librosa.load(file_name)\n",
        "\n",
        "# plot waveform\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.waveshow(audio, sr=sr)\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title('Waveform plot')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
        "  fig, axs = plt.subplots(1, 1)\n",
        "  axs.set_title(title or 'Spectrogram (db)')\n",
        "  axs.set_ylabel(ylabel)\n",
        "  axs.set_xlabel('frame')\n",
        "  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
        "  if xmax:\n",
        "    axs.set_xlim((0, xmax))\n",
        "  fig.colorbar(im, ax=axs)\n",
        "  plt.show(block=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# for instrument in IRMAS_ds.train_data_dict:\n",
        "#   # sample = cycle.next()\n",
        "#   print(instrument)\n",
        "#   file_name = IRMAS_ds.train_data_dict[instrument].next()\n",
        "#   wf, sr = get_wf_sr(file_name)\n",
        "#   mfcc = convert_waveform_to_mfcc(wf, sr)\n",
        "#   plot_spectrogram(mfcc[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIK24SuZYEwS"
      },
      "outputs": [],
      "source": [
        "# #verify there is data in the the test and train loaders, commented out as it takes a long time to run\n",
        "\n",
        "# data, label = next(iter(train_dsLoader_list[0]))\n",
        "# print(f\"train targets: {label}, {data.shape}, {len(data)}\")\n",
        "\n",
        "# data, label = next(iter(val_dsLoader_list[0]))\n",
        "# print(f\"val targets: {label}, {data.shape}, {len(data)}\")\n",
        "\n",
        "# data, label = next(iter(test_dsLoader_list[0]))\n",
        "# print(f\"test targets: {label}, {data.shape}, {len(data)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrt4Jthi3q-t"
      },
      "source": [
        "output_length = (input_length + 2 * padding - dilation * (kernel_size -1) - 1) / stride + 1\n",
        "\n",
        "output_shape = (batch_size, out_channels, output_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAEDHRXHS-xu"
      },
      "outputs": [],
      "source": [
        "class LinearNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 ninputs= 780,   # number of inputs (30x30)\n",
        "                 nhidden=100,   # number of nodes in hidden layer\n",
        "                 nout=11,       # output\n",
        "                 nLayers = 2,\n",
        "                 channels = 1,\n",
        "                 dropout_prob=0.1\n",
        "                ):\n",
        "        super().__init__()\n",
        "        layers = [nn.Linear(ninputs, nhidden), nn.BatchNorm1d(nhidden), nn.ReLU(), nn.Dropout(p=dropout_prob)]\n",
        "\n",
        "        for i in range(nLayers):\n",
        "            layers += [nn.Linear(nhidden, nhidden), nn.BatchNorm1d(nhidden), nn.ReLU(), nn.Dropout(p=dropout_prob)]\n",
        "\n",
        "        layers += [nn.Linear(nhidden, nout, bias=False)]\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # reshape the input tensor to have four dimensions\n",
        "        b, c, h, w = x.shape\n",
        "        x = x.view(b, -1) # Flatten image\n",
        "\n",
        "        # pass the reshaped tensor through the neural network\n",
        "        x = self.net(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pxb4yaEsadJ8"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, inChannels, outChannels, stride=1, downsample=None):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inChannels,\n",
        "                               outChannels,\n",
        "                               kernel_size=3,\n",
        "                               stride=stride,\n",
        "                               padding=1,\n",
        "                               bias=False)\n",
        "        self.bn = nn.BatchNorm2d(outChannels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(outChannels, outChannels, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.downsample = downsample\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_hHpQXouYjM"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=11, dropout_prob=0.5):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inChannels = 64\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, outChannels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inChannels != outChannels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inChannels, outChannels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(outChannels),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inChannels, outChannels, stride, downsample))\n",
        "        self.inChannels = outChannels\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inChannels, outChannels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wvj3fi4gtnKB"
      },
      "outputs": [],
      "source": [
        "class tmp_ds(Dataset):\n",
        "    def __init__(self, val_images, val_labels):\n",
        "        self.images = val_images\n",
        "        self.labels = val_labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        label = self.labels[index]\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjS__CwAqrqc"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm # This is optional but useful\n",
        "import gc\n",
        "\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, model, dataset, num_splits= 20, batch_size=512, num_epochs= 10, num_workers= 1):\n",
        "        #hyper Parameters\n",
        "        self.num_epochs = num_epochs\n",
        "        self.lr = .001\n",
        "        self.batch_size = (batch_size // dataset.shift) * dataset.shift  #ensures that each batch size does not cut between instruments\n",
        "        self.num_splits = num_splits\n",
        "\n",
        "        item = dataset[0]\n",
        "        plot_spectrogram(item[0][0])\n",
        "\n",
        "        self.train_list, self.val_list, self.test_list, self.dataset_copies = split_samples(dataset,\n",
        "                                                                                            num_splits = self.num_splits,\n",
        "                                                                                            batch_size= self.batch_size,\n",
        "                                                                                            num_workers= num_workers)\n",
        "\n",
        "        self.training_data = []\n",
        "        self.training_labels = []\n",
        "        self.testing_data = []\n",
        "        self.testing_labels = []\n",
        "\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.total_loss = []\n",
        "        self.total_val_loss = []\n",
        "\n",
        "        self.reuse_val_dataset = None\n",
        "        self.reuse_val_loader = None\n",
        "\n",
        "        self.num_labels = dataset.get_number_of_labels()\n",
        "        self.num_channels = 0\n",
        "        self.num_frames = 0\n",
        "        self.num_features =  0\n",
        "        self.get_data_info()\n",
        "        self.model = model\n",
        "        #self.model = model(ResBlock, [3,4,6,3], num_classes= num_labels).to(device)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, weight_decay= .001, momentum= .09)\n",
        "        #weight decay is done to prevent overfitting, it does this by adding a penalty to the loss function to make it have smaller values\n",
        "        #momentum helps smooth out the update process and make learning happen quicker\n",
        "\n",
        "    def get_data_info(self):\n",
        "        extract_mfcc_tensor, label = self.dataset[0]\n",
        "        self.num_channels, self.num_frames, self.num_features = extract_mfcc_tensor.shape\n",
        "\n",
        "    def train_over_partition(self, partition_num):\n",
        "        if partition_num > self.num_splits or partition_num < 0:\n",
        "            print(f\"num_partition must be >=0 and  < {self.num_splits}\")\n",
        "        train_loader = self.train_list[partition_num]\n",
        "        valid_loader = self.val_list[partition_num]\n",
        "        test_loader = self.test_list[partition_num]\n",
        "\n",
        "        self.model.train()\n",
        "        loss_history = []\n",
        "        val_loss_history = []\n",
        "        loss = torch.Tensor([0])\n",
        "\n",
        "        start_time = time.time()\n",
        "        reuse_images = []\n",
        "        reuse_labels = []\n",
        "        first_pass = True\n",
        "        for epoch in tqdm(range(self.num_epochs), desc=f\"Epoch\", unit=\"epoch\", disable=False):\n",
        "            #train on train data\n",
        "            for (images, labels) in tqdm(train_loader, desc=\"iteration\", unit=\"%\", disable=True):\n",
        "                self.optimizer.zero_grad(set_to_none=True)\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                loss_history.append(loss.item())\n",
        "                del images, labels, outputs\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "            # Validation\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                for (images, labels) in valid_loader:\n",
        "                    images = images.to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    outputs = self.model(images)\n",
        "                    if first_pass:\n",
        "                        reuse_images.extend(images)\n",
        "                        reuse_labels.extend(labels)\n",
        "                    val_loss = self.criterion(outputs, labels)\n",
        "                    val_loss_history.append(val_loss.item())\n",
        "            first_pass = False\n",
        "            self.model.train()\n",
        "\n",
        "            # Print epoch and losses\n",
        "            avg_train_loss = sum(loss_history) / len(loss_history)\n",
        "            avg_val_loss = sum(val_loss_history) / len(val_loss_history)\n",
        "            print(f\"Epoch {epoch}: train_loss: {avg_train_loss:.5f}, val_loss: {avg_val_loss:.5f}\") # glitch in which it doesn't print last print output\n",
        "\n",
        "        #Train on validation data\n",
        "        print(\"\\nTraining over Validation set:\")\n",
        "        for epoch in tqdm(range(self.num_epochs), desc=f\"Epoch\", unit=\"epoch\", disable=False):\n",
        "            self.reuse_val_data(reuse_images, reuse_labels)\n",
        "            for (images, labels) in tqdm(self.reuse_val_loader, desc=\"iteration\", unit=\"%\", disable=True):\n",
        "                self.optimizer.zero_grad(set_to_none=True)\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                loss_history.append(loss.item())\n",
        "                del images, labels, outputs\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "            avg_train_loss = sum(loss_history) / len(loss_history)\n",
        "            print(f\"Epoch {epoch}: train_loss: {avg_train_loss:.5f}\")\n",
        "        end_time = time.time()\n",
        "        print(f\"Total training time: {(end_time - start_time) / 60:.2f} minutes\")\n",
        "\n",
        "        self.total_loss.extend(loss_history)\n",
        "        self.total_val_loss.extend(val_loss_history)\n",
        "        plt.plot(self.total_loss, label='Training set')\n",
        "        plt.plot(self.total_val_loss, label='Validation set')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "          for data in self.test_list[partition_num]:\n",
        "              inputs, labels = data\n",
        "              inputs = inputs.to(device)\n",
        "              labels = labels.to(device)\n",
        "              outputs = self.model(inputs)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "        print(f\"Accuracy of the network on the test set: {round((100 * correct / total), 2)}%\\n\")\n",
        "\n",
        "    def get_eval_data_over_range(self, start, end):\n",
        "        #collecting training data\n",
        "        print(\"\\nCollecting output data for training\")\n",
        "        with torch.no_grad():\n",
        "          for i in range(start, end):\n",
        "              for data in self.train_list[i]:\n",
        "                  inputs, labels = data\n",
        "                  inputs = inputs.to(device)\n",
        "                  labels = labels.to(device)\n",
        "                  outputs = self.model(inputs)\n",
        "                  self.collect_training_data(outputs, labels)\n",
        "              for data in self.val_list[i]:\n",
        "                  inputs, labels = data\n",
        "                  inputs = inputs.to(device)\n",
        "                  labels = labels.to(device)\n",
        "                  outputs = self.model(inputs)\n",
        "                  self.collect_training_data(outputs, labels)\n",
        "              for data in self.test_list[i]:\n",
        "                  inputs, labels = data\n",
        "                  inputs = inputs.to(device)\n",
        "                  labels = labels.to(device)\n",
        "                  outputs = self.model(inputs)\n",
        "                  self.collect_training_data(outputs, labels, if_test=True)\n",
        "\n",
        "\n",
        "    def collect_training_data(self, outputs, labels, if_test=False):\n",
        "        tmp_list = []\n",
        "        for i, output in enumerate(F.softmax(outputs, dim=1)):\n",
        "            tmp_list.append(output)\n",
        "            if (i + 1) % self.dataset.shift == 0:\n",
        "              if if_test == True:\n",
        "                  self.testing_labels.append(labels[i].to(\"cpu\"))\n",
        "                  self.testing_data.append(torch.stack(tmp_list).unsqueeze(0).to(\"cpu\"))\n",
        "              else:\n",
        "                  self.training_labels.append(labels[i].to(\"cpu\"))\n",
        "                  self.training_data.append(torch.stack(tmp_list).unsqueeze(0).to(\"cpu\"))\n",
        "              tmp_list.clear()\n",
        "\n",
        "\n",
        "    def get_training_data(self):\n",
        "        return self.training_data, self.training_labels, self.testing_data, self.testing_labels\n",
        "\n",
        "\n",
        "    def reuse_val_data(self,images, labels):\n",
        "        self.reuse_val_dataset = tmp_ds(images, labels)\n",
        "        self.reuse_val_loader = DataLoader(self.reuse_val_dataset, batch_size= self.batch_size, shuffle=True)\n",
        "\n",
        "        # temp = list(zip(images, labels))\n",
        "        # random.shuffle(temp)\n",
        "        # r_images, r_labels = zip(*temp)\n",
        "\n",
        "\n",
        "    def only_test(self):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "          for test_partition in self.test_list:\n",
        "            for data in test_partition:\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = self.model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        print(f\"Accuracy of the network on the test set: {round((100 * correct / total), 2)}%\\n\")\n",
        "\n",
        "    def train_over_range(self, start, end):\n",
        "        for i in range(start, end + 1):\n",
        "            print(f\"\\npartition: {i}/{end}\\n\")\n",
        "            self.train_over_partition(i)\n",
        "        self.get_eval_data_over_range(start, end + 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G1zTbH3RwEu"
      },
      "outputs": [],
      "source": [
        "# model = LinearNet().to(device)\n",
        "#model = CNNModel(11).to(device)\n",
        "model = ResNet(ResBlock, [3, 4, 6, 3]).to(device)\n",
        "\n",
        "\n",
        "labels = [\"sax\", \"tru\", \"vio\", \"voi\", \"cel\", \"cla\", \"flu\", \"gac\", \"gel\", \"org\", \"pia\"]\n",
        "IRMAS_ds = IRMASDataset(\"IRMAS-TrainingData\", segment_percent= .1, included_labels= labels)\n",
        "\n",
        "\n",
        "trainer = Trainer(model, IRMAS_ds, num_splits= 1, num_epochs= 30)\n",
        "trainer.train_over_partition(0)\n",
        "#trainer.train_over_range(0,0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rROaVQzh5j5L"
      },
      "outputs": [],
      "source": [
        "data, labels, test_data, test_labels = trainer.get_training_data()\n",
        "results_ds = tmp_ds(data, labels)\n",
        "test_ds = tmp_ds(test_data, test_labels)\n",
        "\n",
        "print(\"checking training data\")\n",
        "print(len(data))\n",
        "print(len(labels))\n",
        "\n",
        "print(\"checking testing data\")\n",
        "print(len(test_data))\n",
        "print(len(test_labels))\n",
        "\n",
        "# print(results_ds[0][0])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TozkAJzq8pfQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "\n",
        "\n",
        "# Define indices for train, validation, and test splits\n",
        "num_samples = len(results_ds)\n",
        "num_test_samples = len(test_ds)\n",
        "\n",
        "train_split = int(num_samples * 0.9)\n",
        "val_split = num_samples - train_split\n",
        "\n",
        "train_indices = list(range(train_split))\n",
        "val_indices = list(range(train_split, train_split + val_split))\n",
        "test_indices = list(range(0, num_test_samples))\n",
        "\n",
        "# Define samplers for each split\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "# Define data loaders for each split\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(results_ds, batch_size=batch_size, sampler=train_sampler)\n",
        "val_loader = DataLoader(results_ds, batch_size=batch_size, sampler=val_sampler)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, sampler=test_sampler)\n",
        "\n",
        "\n",
        "#print(next(iter(train_loader))[0].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUJQHpzz9Kff"
      },
      "outputs": [],
      "source": [
        "\n",
        "# model =  LinearNet(\n",
        "#                  ninputs= 10 * 11,   # number of inputs (30x30)\n",
        "#                  nhidden=100,   # number of nodes in hidden layer\n",
        "#                  nout=11,       # output\n",
        "#                  nLayers = 1,\n",
        "#                  channels = 1,\n",
        "#                  dropout_prob=0.1\n",
        "#                 ).to(device)\n",
        "\n",
        "\n",
        "model = ResNet(ResBlock, [3, 4, 6, 3]).to(device)\n",
        "\n",
        "num_epochs = 100 #50 looks optimal\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr= .001, weight_decay= .001, momentum= .09)\n",
        "\n",
        "model.train()\n",
        "loss_history = []\n",
        "val_loss_history = []\n",
        "loss = torch.Tensor([0])\n",
        "\n",
        "reuse_images = []\n",
        "reuse_labels = []\n",
        "first_pass = True\n",
        "start_time = time.time()\n",
        "for epoch in tqdm(range(num_epochs), desc=f\"Epoch\", unit=\"epoch\", disable=False):\n",
        "    #train on train data\n",
        "    for (images, labels) in tqdm(train_loader, desc=\"iteration\", unit=\"%\", disable=True):\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_history.append(loss.item())\n",
        "        del images, labels, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (images, labels) in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            val_loss_history.append(val_loss.item())\n",
        "    model.train()\n",
        "    avg_train_loss = sum(loss_history) / len(loss_history)\n",
        "    avg_val_loss = sum(val_loss_history) / len(val_loss_history)\n",
        "    print(f\"Epoch {epoch}: train_loss: {avg_train_loss:.5f}, val_loss: {avg_val_loss:.5f}\") # glitch in which it doesn't print last print output\n",
        "\n",
        "#Train on validation data\n",
        "print(\"\\nTraining over Validation set:\")\n",
        "for epoch in tqdm(range(num_epochs//3), desc=f\"Epoch\", unit=\"epoch\", disable=False):\n",
        "    for (images, labels) in tqdm(val_loader, desc=\"iteration\", unit=\"%\", disable=True):\n",
        "        optimizer.zero_grad()\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_history.append(loss.item())\n",
        "        del images, labels, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    avg_train_loss = sum(loss_history) / len(loss_history)\n",
        "    print(f\"Epoch {epoch}: train_loss: {avg_train_loss:.5f}\")\n",
        "\n",
        "#total_loss.extend(loss_history)\n",
        "#total_val_loss.extend(val_loss_history)\n",
        "plt.plot(loss_history, label='Training set')\n",
        "plt.plot(val_loss_history, label='Validation set')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "end_time = time.time() # End time\n",
        "print(f\"Total training time: {(end_time - start_time) / 60:.2f} minutes\")\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for (images, labels) in test_loader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "print(f\"Accuracy of the network on the test set: {round((100 * correct / total), 2)}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for (images, labels) in test_loader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "print(f\"Accuracy of the network on the test set: {round((100 * correct / total), 2)}%\\n\")"
      ],
      "metadata": {
        "id": "yxhCHmZ4pbfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VyA1fVrRpPb"
      },
      "outputs": [],
      "source": [
        "# model = CNN(11).to(device)\n",
        "# trainer = Trainer(model, IRMAS_ds, num_splits= 5, num_epochs= 10)\n",
        "# trainer.train_over_range(0, 4) #error here, original had set to 5, but that would be 6 partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cT1texxR72E"
      },
      "outputs": [],
      "source": [
        "#model = ResNet(ResBlock, [2, 2, 2, 2]).to(device) #68%\n",
        "# labels = [\"sax\", \"tru\", \"vio\", \"voi\", \"cel\", \"cla\", \"flu\", \"gac\", \"gel\", \"org\", \"pia\"]\n",
        "# import torch.nn as nn\n",
        "\n",
        "# import torch.nn as nn\n",
        "\n",
        "# Define the CNN model\n",
        "\n",
        "\n",
        "\n",
        "# train_labels = []\n",
        "# for label in labels:\n",
        "#     train_labels.append(label)\n",
        "#     if len(train_labels) == 2:\n",
        "\n",
        "#         train_labels.clear()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YjhEzXcw3pI"
      },
      "outputs": [],
      "source": [
        "# labels = [\"sax\", \"tru\", \"vio\", \"voi\", \"cel\", \"cla\", \"flu\", \"gac\", \"gel\", \"org\"] # no piano\n",
        "# IRMAS_ds = IRMASDataset(\"IRMAS-TrainingData\", included_labels= labels)\n",
        "# trainer = Trainer(model, IRMAS_ds, num_splits= 10, num_epochs= 10)\n",
        "# trainer.only_test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSsu4PENfq9u"
      },
      "outputs": [],
      "source": [
        "\n",
        "        # temp = list(zip(images, labels))\n",
        "        # random.shuffle(temp)\n",
        "        # r_images, r_labels = zip(*temp)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}